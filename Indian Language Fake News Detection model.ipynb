{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-20T09:26:30.702555Z",
     "iopub.status.busy": "2024-11-20T09:26:30.702138Z",
     "iopub.status.idle": "2024-11-20T09:26:30.723551Z",
     "shell.execute_reply": "2024-11-20T09:26:30.722141Z",
     "shell.execute_reply.started": "2024-11-20T09:26:30.702521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T09:26:32.916729Z",
     "iopub.status.busy": "2024-11-20T09:26:32.916292Z",
     "iopub.status.idle": "2024-11-20T09:27:11.217491Z",
     "shell.execute_reply": "2024-11-20T09:27:11.215971Z",
     "shell.execute_reply.started": "2024-11-20T09:26:32.916696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install optuna\n",
    "!pip install tqdm\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T10:00:40.695571Z",
     "iopub.status.busy": "2024-11-20T10:00:40.694984Z",
     "iopub.status.idle": "2024-11-20T10:01:30.499750Z",
     "shell.execute_reply": "2024-11-20T10:01:30.498596Z",
     "shell.execute_reply.started": "2024-11-20T10:00:40.695531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "true_data = pd.read_csv('/kaggle/input/hindi-and-english/True.csv')\n",
    "merged_data = pd.read_csv('/kaggle/input/hindi-and-english/dataset-merged.csv')\n",
    "bang_data = pd.read_csv('/kaggle/input/bangla-news/balanced_bn_data.csv')\n",
    "guja_data = pd.read_csv('/kaggle/input/gujarati-news/gujarati_news.csv')\n",
    "mara_data = pd.read_csv('/kaggle/input/marathi-news-tf/marathi_news.csv')\n",
    "telu_data = pd.read_csv('/kaggle/input/telugu/telugu_news.csv')\n",
    "# Preprocess data\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \"\", text)  # Keep only alphanumeric and Hindi/Bengali characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespaces\n",
    "    text = text.lower()  # Convert to lowercase for English\n",
    "   \n",
    "    return text\n",
    "\n",
    "true_data['feature'] = (true_data['title'].fillna(\"\") + \" \" + true_data['text'].fillna(\"\")).apply(clean_text)\n",
    "merged_data['feature'] = merged_data['text'].fillna(\"\").apply(clean_text)\n",
    "bang_data['feature'] = bang_data['content'].fillna(\"\").apply(clean_text)\n",
    "guja_data['feature'] = guja_data['text'].fillna(\"\").apply(clean_text)\n",
    "mara_data['feature'] = mara_data['text'].fillna(\"\").apply(clean_text)\n",
    "telu_data['feature'] = telu_data['text'].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# Combine datasets\n",
    "data = pd.concat([true_data[['feature', 'label']], merged_data[['feature', 'label']],bang_data[['feature','label']],guja_data[['feature','label']],mara_data[['feature','label']],telu_data[['feature','label']]])\n",
    "data = data.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T10:02:57.772995Z",
     "iopub.status.busy": "2024-11-20T10:02:57.772580Z",
     "iopub.status.idle": "2024-11-20T10:02:58.176911Z",
     "shell.execute_reply": "2024-11-20T10:02:58.175776Z",
     "shell.execute_reply.started": "2024-11-20T10:02:57.772959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot the data distribution\n",
    "def plot_data_distribution(data):\n",
    "    # Plot the total number of labels (0 and 1) in the combined dataset\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Barplot for label distribution\n",
    "    sns.countplot(x='label', data=data, palette='Set2')\n",
    "    plt.title('Distribution of Labels (0 and 1)')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the total number of data points per dataset\n",
    "    sources = ['English (True.csv)', 'Hindi (dataset-merged.csv)', 'Bangla (balanced_bn_data.csv)','Gujarati (gujarati_news.csv)','Marathi(marathi_news.csv)','Telugu(telugu_news.csv)']\n",
    "    counts = [len(true_data), len(merged_data), len(bang_data), len(guja_data),len(mara_data),len(telu_data)]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=sources, y=counts, palette='Set3')\n",
    "    plt.title('Number of Data Points per Dataset')\n",
    "    plt.xlabel('Dataset Source')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the overall summary of combined data\n",
    "    print(f\"Total Data Points in Combined Dataset: {len(data)}\")\n",
    "    print(f\"Number of Label 0: {len(data[data['label'] == 0])}\")\n",
    "    print(f\"Number of Label 1: {len(data[data['label'] == 1])}\")\n",
    "\n",
    "# Call the function to plot\n",
    "plot_data_distribution(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T09:51:38.241323Z",
     "iopub.status.busy": "2024-11-20T09:51:38.240431Z",
     "iopub.status.idle": "2024-11-20T09:51:39.598596Z",
     "shell.execute_reply": "2024-11-20T09:51:39.597539Z",
     "shell.execute_reply.started": "2024-11-20T09:51:38.241279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_individual_label_distribution(true_data, merged_data, bang_data, guja_data, mara_data, telu_data):\n",
    "    # Plot label distribution for English dataset (True.csv)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=true_data, palette='coolwarm')\n",
    "    plt.title('Label Distribution in English Dataset (True.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot label distribution for Hindi dataset (dataset-merged.csv)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=merged_data, palette='viridis')\n",
    "    plt.title('Label Distribution in Hindi Dataset (dataset-merged.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=bang_data, palette='viridis')\n",
    "    plt.title('Label Distribution in Bangla Dataset (balanced_bn_data.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=guja_data, palette='viridis')\n",
    "    plt.title('Label Distribution in Gujarati Dataset (gujarati_news.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=mara_data, palette='viridis')\n",
    "    plt.title('Label Distribution in Marathi Dataset (marathi_news.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='label', data=telu_data, palette='viridis')\n",
    "    plt.title('Label Distribution in Telugu Dataset (telugu_news.csv)')\n",
    "    plt.xlabel('Label (0: Fake, 1: True)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Print summary information\n",
    "    print(f\"English Dataset (True.csv):\\nLabel 0: {len(true_data[true_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(true_data[true_data['label'] == 1])}\\n\")\n",
    "    print(f\"Hindi Dataset (dataset-merged.csv):\\nLabel 0: {len(merged_data[merged_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(merged_data[merged_data['label'] == 1])}\\n\")\n",
    "    print(f\"Bangle Dataset (balanced_bn_data.csv):\\nLabel 0: {len(bang_data[bang_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(bang_data[bang_data['label'] == 1])}\")\n",
    "    print(f\"Gujarati Dataset (gujarati_news.csv):\\nLabel 0: {len(guja_data[guja_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(guja_data[guja_data['label'] == 1])}\")\n",
    "    print(f\"Marathi Dataset (marathi_news.csv):\\nLabel 0: {len(mara_data[mara_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(mara_data[mara_data['label'] == 1])}\")\n",
    "    print(f\"Telugu Dataset (telugu_news.csv):\\nLabel 0: {len(telu_data[telu_data['label'] == 0])}, \"\n",
    "          f\"Label 1: {len(telu_data[telu_data['label'] == 1])}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the function to generate the plots\n",
    "plot_individual_label_distribution(true_data, merged_data, bang_data, guja_data, mara_data, telu_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Storing datasets in a dictionary for easy access\n",
    "datasets = {\n",
    "    'English Data': true_data,\n",
    "    'Hindi Data': merged_data,\n",
    "    'Bangla Data': bang_data,\n",
    "    'Gujarati Data': guja_data,\n",
    "    'Marathi Data': mara_data,\n",
    "    'Telugu Data': telu_data\n",
    "}\n",
    "\n",
    "# Visualization of label distributions\n",
    "def plot_label_distribution(datasets):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, data) in enumerate(datasets.items()):\n",
    "        label_counts = data['label'].value_counts()\n",
    "        sns.barplot(x=label_counts.index, y=label_counts.values, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f\"{name} Label Distribution\")\n",
    "        axes[i].set_xlabel('Labels')\n",
    "        axes[i].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"label_distribution.jpg\", format='jpg', dpi=300)  # Save plot as JPG\n",
    "    plt.show()\n",
    "def plot_combined_distribution(datasets):\n",
    "    combined_data = []\n",
    "    for name, data in datasets.items():\n",
    "        label_counts = data['label'].value_counts().reset_index()\n",
    "        label_counts.columns = ['Label', 'Count']\n",
    "        label_counts['Dataset'] = name\n",
    "        combined_data.append(label_counts)\n",
    "    \n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=combined_df, x='Dataset', y='Count', hue='Label', palette='tab10')\n",
    "    plt.title(\"Combined Label Distribution Across Datasets\")\n",
    "    plt.xticks(rotation=35)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Labels')\n",
    "    plt.savefig(\"combined_label_distribution.jpg\", format='jpg', dpi=550)  # Save plot as JPG\n",
    "    plt.show()\n",
    "\n",
    "# Pie charts for label proportions\n",
    "def plot_pie_charts(datasets):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, data) in enumerate(datasets.items()):\n",
    "        label_counts = data['label'].value_counts()\n",
    "        axes[i].pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', colors=sns.color_palette('viridis', len(label_counts)))\n",
    "        axes[i].set_title(f\"{name} Label Proportion\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"label_proportions.jpg\", format='jpg', dpi=300)  # Save plot as JPG\n",
    "    plt.show()\n",
    "\n",
    "# Displaying and saving visualizations\n",
    "plot_label_distribution(datasets)\n",
    "plot_combined_distribution(datasets)\n",
    "plot_pie_charts(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:17:48.736299Z",
     "iopub.status.busy": "2024-11-20T05:17:48.735797Z",
     "iopub.status.idle": "2024-11-20T05:20:38.778404Z",
     "shell.execute_reply": "2024-11-20T05:20:38.777151Z",
     "shell.execute_reply.started": "2024-11-20T05:17:48.736254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Tokenize data\n",
    "MAX_LENGTH = min(512, int(data['feature'].apply(lambda x: len(x.split())).quantile(0.9)))\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "    data['feature'].tolist(),\n",
    "    max_length=MAX_LENGTH,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:24:35.631916Z",
     "iopub.status.busy": "2024-11-20T05:24:35.629710Z",
     "iopub.status.idle": "2024-11-20T05:24:35.664610Z",
     "shell.execute_reply": "2024-11-20T05:24:35.663345Z",
     "shell.execute_reply.started": "2024-11-20T05:24:35.631857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:25:09.295591Z",
     "iopub.status.busy": "2024-11-20T05:25:09.294251Z",
     "iopub.status.idle": "2024-11-20T05:25:09.302248Z",
     "shell.execute_reply": "2024-11-20T05:25:09.300648Z",
     "shell.execute_reply.started": "2024-11-20T05:25:09.295525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data_loader(indices):\n",
    "    return DataLoader(TensorDataset(*[x[indices] for x in dataset.tensors]), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:25:10.973003Z",
     "iopub.status.busy": "2024-11-20T05:25:10.972569Z",
     "iopub.status.idle": "2024-11-20T05:25:10.982628Z",
     "shell.execute_reply": "2024-11-20T05:25:10.981342Z",
     "shell.execute_reply.started": "2024-11-20T05:25:10.972970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class XLMRobertaClassifier(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(XLMRobertaClassifier, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 512)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 384)  # New layer: 512 → 384\n",
    "        self.fc3 = nn.Linear(384, 2)    # Output layer: 384 → 2\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass input through the RoBERTa model\n",
    "        _, pooled_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        \n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        x = self.dropout(self.relu(self.fc1(pooled_output)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))  # New layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Apply softmax activation for classification\n",
    "        return self.softmax(x).float()  # Ensure output is float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:25:14.506149Z",
     "iopub.status.busy": "2024-11-20T05:25:14.504932Z",
     "iopub.status.idle": "2024-11-20T05:25:22.277581Z",
     "shell.execute_reply": "2024-11-20T05:25:22.276638Z",
     "shell.execute_reply.started": "2024-11-20T05:25:14.506085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = XLMRobertaClassifier('xlm-roberta-base').to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T13:01:30.596735Z",
     "iopub.status.busy": "2024-11-17T13:01:30.595853Z",
     "iopub.status.idle": "2024-11-17T13:01:30.610111Z",
     "shell.execute_reply": "2024-11-17T13:01:30.609392Z",
     "shell.execute_reply.started": "2024-11-17T13:01:30.596690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#path = '/kaggle/input/fake-news-detection-model/pytorch/default/1/best_model_last2.pt'\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T17:09:23.222472Z",
     "iopub.status.busy": "2024-11-18T17:09:23.221908Z",
     "iopub.status.idle": "2024-11-18T17:09:23.226694Z",
     "shell.execute_reply": "2024-11-18T17:09:23.225850Z",
     "shell.execute_reply.started": "2024-11-18T17:09:23.222436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T10:27:14.351183Z",
     "iopub.status.busy": "2024-11-19T10:27:14.350556Z",
     "iopub.status.idle": "2024-11-19T10:27:14.369147Z",
     "shell.execute_reply": "2024-11-19T10:27:14.368430Z",
     "shell.execute_reply.started": "2024-11-19T10:27:14.351151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T05:24:47.013608Z",
     "iopub.status.busy": "2024-11-20T05:24:47.013141Z",
     "iopub.status.idle": "2024-11-20T05:24:47.390773Z",
     "shell.execute_reply": "2024-11-20T05:24:47.389247Z",
     "shell.execute_reply.started": "2024-11-20T05:24:47.013570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T10:27:26.024711Z",
     "iopub.status.busy": "2024-11-19T10:27:26.023915Z",
     "iopub.status.idle": "2024-11-19T10:29:36.682596Z",
     "shell.execute_reply": "2024-11-19T10:29:36.681445Z",
     "shell.execute_reply.started": "2024-11-19T10:27:26.024678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_loader)\n",
    "    milestones = [int(num_batches * 0.25), int(num_batches * 0.5), int(num_batches * 0.75), num_batches - 1]\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        labels = labels.long()  # Ensure labels are int64\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress only at milestones\n",
    "        if step in milestones:\n",
    "            print(f\"Training Progress: {((step + 1) / num_batches) * 100:.0f}% completed | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    preds = []  # Initialize predictions list\n",
    "    true_labels = []  # Initialize true labels list\n",
    "    num_batches = len(val_loader)\n",
    "    milestones = [int(num_batches * 0.25), int(num_batches * 0.5), int(num_batches * 0.75), num_batches - 1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            labels = labels.long()  # Ensure labels are int64\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Print progress only at milestones\n",
    "            if step in milestones:\n",
    "                print(f\"Evaluation Progress: {((step + 1) / num_batches) * 100:.0f}% completed | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    return total_loss / num_batches, accuracy\n",
    "\n",
    "# Training loop with progress tracking\n",
    "epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train_loss = train()  # Train the model\n",
    "    valid_loss, valid_acc = evaluate()  # Evaluate accuracy after the epoch\n",
    "    scheduler.step(valid_loss)  # Update learning rate based on validation loss\n",
    "\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {valid_acc:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_last3.pt')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_last3.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:27:22.679925Z",
     "iopub.status.busy": "2024-10-10T13:27:22.679514Z",
     "iopub.status.idle": "2024-10-10T13:27:22.691309Z",
     "shell.execute_reply": "2024-10-10T13:27:22.690408Z",
     "shell.execute_reply.started": "2024-10-10T13:27:22.679869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:27:22.692613Z",
     "iopub.status.busy": "2024-10-10T13:27:22.692340Z",
     "iopub.status.idle": "2024-10-10T13:28:07.467904Z",
     "shell.execute_reply": "2024-10-10T13:28:07.466625Z",
     "shell.execute_reply.started": "2024-10-10T13:27:22.692583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# best_valid_loss = float('inf')\n",
    "# train_losses = []\n",
    "# valid_losses = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "#     train_loss = train()\n",
    "#     print(\"Training done\")\n",
    "#     valid_loss = evaluate()\n",
    "#     print(\"Validation done\")\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'c5_new_model_weights.pt')\n",
    "#     train_losses.append(train_loss)\n",
    "#     valid_losses.append(valid_loss)\n",
    "\n",
    "#     print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "#     print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T10:31:56.256379Z",
     "iopub.status.busy": "2024-11-19T10:31:56.256065Z",
     "iopub.status.idle": "2024-11-19T10:31:56.263636Z",
     "shell.execute_reply": "2024-11-19T10:31:56.262714Z",
     "shell.execute_reply.started": "2024-11-19T10:31:56.256353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(test_loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.append(preds)\n",
    "        true_labels.append(labels)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    preds_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, preds_labels)\n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds_labels, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "#     print(f'Precision: {precision:.4f}')\n",
    "#     print(f'Recall: {recall:.4f}')\n",
    "#     print(f'F1-Score: {f1:.4f}')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Run inference after training\n",
    "#accuracy, precision, recall, f1 = inference(val_loader)\n",
    "accuracy = inference(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T10:32:04.646853Z",
     "iopub.status.busy": "2024-11-19T10:32:04.646142Z",
     "iopub.status.idle": "2024-11-19T10:32:04.671580Z",
     "shell.execute_reply": "2024-11-19T10:32:04.670503Z",
     "shell.execute_reply.started": "2024-11-19T10:32:04.646818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = inference(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5846996,
     "sourceId": 9587530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6114150,
     "sourceId": 9943663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6119012,
     "sourceId": 9950178,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6126624,
     "sourceId": 9960607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6126886,
     "sourceId": 9960958,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 170308,
     "modelInstanceId": 147780,
     "sourceId": 173612,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
